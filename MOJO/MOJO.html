
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>We are More than Our Joints: Predicting how 3D Bodies Move</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="A key step towards understanding human behavior is the prediction of 3D human motion. Successful solutions have many applications in human tracking, HCI, and graphics. Most previous work focuses on predicting a time series of future 3D joint locations given a sequence 3D joints from the past. This Euclidean formulation generally works better than predicting pose in terms of joint rotations. Body joint locations, however, do not fully constrain 3D human pose, leaving degrees of freedom (like rotation about a limb) undefined, making it hard to animate a realistic human from only the joints. Note that the 3D joints can be viewed as a sparse {\em point cloud}. Thus the problem of human motion prediction can be seen as a problem of point cloud prediction. With this observation, we instead predict a sparse set of locations on the body {\em surface} that correspond to motion capture markers. Given such markers, we fit a parametric body model to recover the 3D body shape and pose of the person. These sparse surface markers also carry detailed information about human movement that is not present in the joints, increasing the naturalness of the predicted motions. Using the AMASS dataset, we train MOJO (More than Our JOints), which is a novel variational autoencoder with a latent DCT space that generates motions from latent frequencies. MOJO preserves the full temporal resolution of the input motion, and sampling from the latent frequencies explicitly introduces high-frequency components into the generated motion. We note that motion prediction methods accumulate errors over time, resulting in joints or markers that diverge from true human bodies. To address this, we exploit the body model and fit SMPL-X to the predictions at each time step, projecting the solution back onto the space of valid bodies. These valid markers are then propagated in time. Quantitative and qualitative experiments show that our approach produces state-of-the-art results and realistic 3D body animations. The code is available for research purposes at https://yz-cnsdqz.github.io/MOJO/MOJO.html ">
<meta name="keywords" content="3D body, stochastic motion prediction">
<!--<link rel="author" href="http://wywu.github.io">-->

<!-- Fonts and stuff -->
<link href="./support/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./support/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./support/iconize.css">
<script async="" src="./support/prettify.js"></script>


</head>


<body>
  <div id="content">
    <div id="content-inner">

		<div class="section head">
	<h1><font size="5">We are More than Our Joints: Predicting how 3D Bodies Move</font></h1>

	<div class="authors">
		<a href="https://inf.ethz.ch/people/people-atoz/person-detail.MjcwNjU2.TGlzdC8zMDQsLTIxNDE4MTU0NjA=.html">Yan Zhang</a><sup>1</sup>
	  	<a href="https://ps.is.tuebingen.mpg.de/person/black">Michael J. Black</a><sup>2</sup>&nbsp;
	  	<a href="https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html">Siyu Tang</a><sup>1</sup>
	</div>

	<div class="affiliations">
	  	<sup>1</sup><a href="https://ethz.ch/en.html">ETH Zurich<br></a>
	  	<sup>2</sup><a href="https://is.mpg.de/">Max Planck Institute for Intelligent Systems<br></a>
	</div>

	<!-- <div class="venue">International Conference on 3D Vision (<a href="http://3dv2020.dgcv.nii.ac.jp/" target="_blank">3DV</a>) 2020 </div> -->

	<ul id="tabs">
		<li><a href="./MOJO.html" name="#tab1">MOJO</a></li>
	</ul>
	</div>
      <center><img src="./support/MOJO-teaser.png" border="0" width="90%"></center>
    <div class="section abstract">
	<h2>Abstract</h2>
	<p>
A key step towards understanding human behavior is the prediction of 3D human motion. Successful solutions have many applications in human tracking, HCI, and graphics. Most previous work focuses on predicting a time series of future 3D joint locations given a sequence 3D joints from the past. This Euclidean formulation generally works better than predicting pose in terms of joint rotations. Body joint locations, however, do not fully constrain 3D human pose, leaving degrees of freedom (like rotation about a limb) undefined, making it hard to animate a realistic human from only the joints. Note that the 3D joints can be viewed as a sparse <i>point cloud</i>. Thus the problem of human motion prediction can be seen as a problem of point cloud prediction. With this observation, we instead predict a sparse set of locations on the body <i>surface</i> that correspond to motion capture markers. Given such markers, we fit a parametric body model to recover the 3D body shape and pose of the person. These sparse surface markers also carry detailed information about human movement that is not present in the joints, increasing the naturalness of the predicted motions. Using the AMASS dataset, we train MOJO (More than Our JOints), which is a novel variational autoencoder with a latent DCT space that generates motions from latent frequencies. MOJO preserves the full temporal resolution of the input motion, and sampling from the latent frequencies explicitly introduces high-frequency components into the generated motion. We note that motion prediction methods accumulate errors over time, resulting in joints or markers that diverge from true human bodies. To address this, we exploit the body model and fit SMPL-X to the predictions at each time step, projecting the solution back onto the space of valid bodies. These valid markers are then propagated in time. Quantitative and qualitative experiments show that our approach produces state-of-the-art results and realistic 3D body animations. The code is available for research purposes at https://yz-cnsdqz.github.io/MOJO/MOJO.html
	</p>
    </div>
<div class="section downloads">
	<h2>Videos</h2><center>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/5DqLWAb37X0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>	
	</center></div>
		<!-- <center>
	  <ul>
        <li class="grid">
	      <div class="griditem">
		<a href="https://www.youtube.com/watch?v=0K6ThmvYFgw" target="_blank" class="imageLink"><img src="./support/video.png"></a><br><a href="https://www.youtube.com/watch?v=0K6ThmvYFgw">Short video (2 min)</a>
		</div>
	      </li>
	    <li class="grid">
	      <div class="griditem">
		<a href="https://www.youtube.com/watch?v=zJ1hbtMHGrw" target="_blank" class="imageLink"><img src="./support/video.png"></a><br><a href="https://www.youtube.com/watch?v=zJ1hbtMHGrw" target="_blank">Long video (7 min)</a>
		</div>
	      </li>
		  </ul>
	    </center>
		</div>-->

<div class="section downloads">
<h2>Downloads</h2><center>
<div class="container">
	<p>
	<a href="https://arxiv.org/abs/2012.00619"><button type="button" class="btn-slide-line center">
		<span>Paper</span>
	</button></a>
	<a href=""><button type="button" class="btn-slide-line center">
		<span>Code</span>
	</button></a>
	</p>
	</div>
</center>
</div>

<br>
 <!-- <div class="section list">
	<h2>Citation</h2>
	
	<div class="section bibtex">
	  <pre>@inproceedings{PLACE:3DV:2020,
  title = {{PLACE}: Proximity Learning of Articulation and Contact in {3D} Environments},
  author = {Zhang, Siwei and Zhang, Yan and Ma, Qianli and Black, Michael J. and Tang, Siyu},
  booktitle = {International Conference on 3D Vision (3DV)},
  month = nov,
  year = {2020},
  month_numeric = {11}
}
	</pre>
	  </div>
      </div> -->

     <div class="section contact">
	<h2>Contact</h2>
		 For questions, please contact Yan Zhang:<br><a href="mailto:yan.zhang@inf.ethz.ch">yan.zhang@inf.ethz.ch</a>
      </div>
    </div>
  </div>

</body></html>

