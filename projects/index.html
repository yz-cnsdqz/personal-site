<!DOCTYPE html><html itemscope itemtype="http://schema.org/WebPage" lang="en"><head><meta content="text/html;charset=utf-8" http-equiv="content-type"><meta content="IE=edge" http-equiv="X-UA-Compatible"><meta content="width=device-width,initial-scale=1" name="viewport" id="viewport"><style type="text/css">svg:not(:root).svg-inline--fa{overflow:visible}.svg-inline--fa{display:inline-block;font-size:inherit;height:1em;overflow:visible;vertical-align:-.125em}.svg-inline--fa.fa-lg{vertical-align:-.225em}.svg-inline--fa.fa-w-1{width:.0625em}.svg-inline--fa.fa-w-2{width:.125em}.svg-inline--fa.fa-w-3{width:.1875em}.svg-inline--fa.fa-w-4{width:.25em}.svg-inline--fa.fa-w-5{width:.3125em}.svg-inline--fa.fa-w-6{width:.375em}.svg-inline--fa.fa-w-7{width:.4375em}.svg-inline--fa.fa-w-8{width:.5em}.svg-inline--fa.fa-w-9{width:.5625em}.svg-inline--fa.fa-w-10{width:.625em}.svg-inline--fa.fa-w-11{width:.6875em}.svg-inline--fa.fa-w-12{width:.75em}.svg-inline--fa.fa-w-13{width:.8125em}.svg-inline--fa.fa-w-14{width:.875em}.svg-inline--fa.fa-w-15{width:.9375em}.svg-inline--fa.fa-w-16{width:1em}.svg-inline--fa.fa-w-17{width:1.0625em}.svg-inline--fa.fa-w-18{width:1.125em}.svg-inline--fa.fa-w-19{width:1.1875em}.svg-inline--fa.fa-w-20{width:1.25em}.svg-inline--fa.fa-pull-left{margin-right:.3em;width:auto}.svg-inline--fa.fa-pull-right{margin-left:.3em;width:auto}.svg-inline--fa.fa-border{height:1.5em}.svg-inline--fa.fa-li{width:2em}.svg-inline--fa.fa-fw{width:1.25em}.fa-layers svg.svg-inline--fa{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.fa-layers{display:inline-block;height:1em;position:relative;text-align:center;vertical-align:-.125em;width:1em}.fa-layers svg.svg-inline--fa{-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter,.fa-layers-text{display:inline-block;position:absolute;text-align:center}.fa-layers-text{left:50%;top:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter{background-color:#ff253a;border-radius:1em;-webkit-box-sizing:border-box;box-sizing:border-box;color:#fff;height:1.5em;line-height:1;max-width:5em;min-width:1.5em;overflow:hidden;padding:.25em;right:0;text-overflow:ellipsis;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-bottom-right{bottom:0;right:0;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom right;transform-origin:bottom right}.fa-layers-bottom-left{bottom:0;left:0;right:auto;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom left;transform-origin:bottom left}.fa-layers-top-right{right:0;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-top-left{left:0;right:auto;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top left;transform-origin:top left}.fa-lg{font-size:1.3333333333em;line-height:.75em;vertical-align:-.0667em}.fa-xs{font-size:.75em}.fa-sm{font-size:.875em}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:2.5em;padding-left:0}.fa-ul>li{position:relative}.fa-li{left:-2em;position:absolute;text-align:center;width:2em;line-height:inherit}.fa-border{border:solid .08em #eee;border-radius:.1em;padding:.2em .25em .15em}.fa-pull-left{float:left}.fa-pull-right{float:right}.fa.fa-pull-left,.fab.fa-pull-left,.fal.fa-pull-left,.far.fa-pull-left,.fas.fa-pull-left{margin-right:.3em}.fa.fa-pull-right,.fab.fa-pull-right,.fal.fa-pull-right,.far.fa-pull-right,.fas.fa-pull-right{margin-left:.3em}.fa-spin{-webkit-animation:fa-spin 2s infinite linear;animation:fa-spin 2s infinite linear}.fa-pulse{-webkit-animation:fa-spin 1s infinite steps(8);animation:fa-spin 1s infinite steps(8)}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.fa-rotate-90{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-webkit-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{-webkit-transform:scale(1,-1);transform:scale(1,-1)}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1,-1);transform:scale(-1,-1)}:root .fa-flip-both,:root .fa-flip-horizontal,:root .fa-flip-vertical,:root .fa-rotate-180,:root .fa-rotate-270,:root .fa-rotate-90{-webkit-filter:none;filter:none}.fa-stack{display:inline-block;height:2em;position:relative;width:2.5em}.fa-stack-1x,.fa-stack-2x{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.svg-inline--fa.fa-stack-1x{height:1em;width:1.25em}.svg-inline--fa.fa-stack-2x{height:2em;width:2.5em}.fa-inverse{color:#fff}.sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.sr-only-focusable:active,.sr-only-focusable:focus{clip:auto;height:auto;margin:0;overflow:visible;position:static;width:auto}.svg-inline--fa .fa-primary{fill:var(--fa-primary-color,currentColor);opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa .fa-secondary{fill:var(--fa-secondary-color,currentColor);opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-primary{opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-secondary{opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa mask .fa-primary,.svg-inline--fa mask .fa-secondary{fill:#000}.fad.fa-inverse{color:#fff}</style><link href="/personal-site/images/favicon/apple-icon-57x57.png" rel="apple-touch-icon" sizes="57x57"><link href="/personal-site/images/favicon/apple-icon-60x60.png" rel="apple-touch-icon" sizes="60x60"><link href="/personal-site/images/favicon/apple-icon-72x72.png" rel="apple-touch-icon" sizes="72x72"><link href="/personal-site/images/favicon/apple-icon-76x76.png" rel="apple-touch-icon" sizes="76x76"><link href="/personal-site/images/favicon/apple-icon-114x114.png" rel="apple-touch-icon" sizes="114x114"><link href="/personal-site/images/favicon/apple-icon-120x120.png" rel="apple-touch-icon" sizes="120x120"><link href="/personal-site/images/favicon/apple-icon-144x144.png" rel="apple-touch-icon" sizes="144x144"><link href="/personal-site/images/favicon/apple-icon-152x152.png" rel="apple-touch-icon" sizes="152x152"><link href="/personal-site/images/favicon/apple-icon-180x180.png" rel="apple-touch-icon" sizes="180x180"><link href="/personal-site/images/favicon/android-icon-192x192.png" rel="icon" sizes="192x192" type="image/png"><link href="/personal-site/images/favicon/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"><link href="/personal-site/images/favicon/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png"><link href="/personal-site/images/favicon/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"><link href="/personal-site/images/favicon/manifest.json" rel="manifest"><meta content="#ffffff" name="msapplication-TileColor"><meta content="/personal-site/images/favicon/ms-icon-144x144.png" name="msapplication-TileImage"><meta content="#ffffff" name="theme-color"><link href="/personal-site/" rel="canonical"><link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Raleway:400,800,900" rel="stylesheet" async><link href="/personal-site/static/css/main.df23277b.chunk.css" rel="stylesheet"><link href="http://fonts.googleapis.com" rel="preconnect"><script src="/personal-site/static/js/6.5f952de4.chunk.js" charset="utf-8"></script><script src="/personal-site/static/js/4.a0c4899c.chunk.js" charset="utf-8"></script><title>Yan Zhang</title><link href="https://www.google-analytics.com" rel="preconnect"><link href="http://fonts.gstatic.com" rel="preconnect"></head><body><div id="root"><div id="wrapper"><header id="header"><h1 class="index-link"><a href="/personal-site/">Yan Zhang</a></h1><nav class="links"><ul><li><a href="/personal-site/about">About</a></li><li><a href="/personal-site/resume">Resume</a></li><li><a href="/personal-site/projects">Publications</a></li><li><a href="/personal-site/blog">Blog</a></li><li><a href="/personal-site/contact">Contact</a></li></ul></nav><div class="hamburger-container"><nav class="main" id="hambuger-nav"><ul><li class="menu open-menu"><div class="menu-hover">â˜°</div></li></ul></nav><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0,0,0,.3);opacity:0;transform:translate3d(100%,0,0);transition:opacity .3s ease 0s,transform 0s ease .3s"></div><div><div class="bm-burger-button" style="z-index:1000"><button id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:0 0;cursor:pointer">Open Menu</button><span><span style="position:absolute;height:20%;left:0;right:0;top:0;opacity:1" class="bm-burger-bars"></span><span style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1" class="bm-burger-bars"></span><span style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1" class="bm-burger-bars"></span></span></div></div><div class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;transform:translate3d(100%,0,0);transition:all .5s ease 0s" aria-hidden="true" id=""><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto"><nav class="bm-item-list" style="height:100%"><ul class="bm-item hamburger-ul" style="display:block" tabindex="-1"><li><a href="/personal-site/"><h3 class="index-li">Yan Zhang</h3></a></li><li><a href="/personal-site/about"><h3>About</h3></a></li><li><a href="/personal-site/resume"><h3>Resume</h3></a></li><li><a href="/personal-site/projects"><h3>Publications</h3></a></li><li><a href="/personal-site/blog"><h3>Blog</h3></a></li><li><a href="/personal-site/contact"><h3>Contact</h3></a></li></ul></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:0 0;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span style="position:absolute;width:3px;height:14px;transform:rotate(45deg)" class="bm-cross"></span><span style="position:absolute;width:3px;height:14px;transform:rotate(-45deg)" class="bm-cross"></span></span></div></div></div></div></div></header><div id="main"><article class="post" id="projects"><header><div class="title"><h2 data-testid="heading"><a href="/personal-site/projects">Publications</a></h2><p>A full list is on google scholar as well.</p></div></header><div class="cell-container"><article class="mini-post"><header><h3><a href="https://yz-cnsdqz.github.io/MOJO/MOJO.html">We are More than Our Joints: Predicting how 3D Bodies Move [CVPR'21]</a></h3><time class="published">March, 2021</time></header><a href="https://yz-cnsdqz.github.io/MOJO/MOJO.html" class="image"><img alt="We are More than Our Joints: Predicting how 3D Bodies Move [CVPR'21]" src="/personal-site/images/projects/MOJO-teaser.png"></a><div class="description"><p>Contributions:(1) A new marker-based 3D body representation.(2) VAE with latent DCT frequencies for motion modeling.(3) Re-projection scheme to eliminate RNN accumulation error.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a href="https://neuralbodies.github.io/LEAP/">LEAP: Learning Articulated Occupancy of People [CVPR'21]</a></h3><time class="published">March, 2021</time></header><a href="https://neuralbodies.github.io/LEAP/" class="image"><img alt="LEAP: Learning Articulated Occupancy of People [CVPR'21]" src="/personal-site/images/projects/LEAP-teaser.png"></a><div class="description"><p>Contributions:(1) Fast occupency query of a 3D human body.(2) Learned linear blend skinning weights.(3) Differentiable occupency modeling to solve mesh inter-penetration.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a href="https://aptx4869lm.github.io/4DEgocentricBodyCapture/">4D Human Body Capture from Egocentric Video via 3D Scene Grounding [arxiv'21]</a></h3><time class="published">February, 2021</time></header><a href="https://aptx4869lm.github.io/4DEgocentricBodyCapture/" class="image"><img alt="4D Human Body Capture from Egocentric Video via 3D Scene Grounding [arxiv'21]" src="/personal-site/images/projects/FPVCapture.jpg"></a><div class="description"><p>Contributions:(1) 3D body and scene capture from a in-the-wild video.(2) Person-ground interaction modelling</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>Perpetual Motion: Generating Unbounded Human Motion [arxiv'20]</a></h3><time class="published">June, 2020</time></header><a class="image"><img alt="Perpetual Motion: Generating Unbounded Human Motion [arxiv'20]" src="/personal-site/images/projects/PM-teaser.png"></a><div class="description"><p>Contributions: (1) A conditional VAE to couple global trajectory and local pose.(2) A robust KL-divergence term to avoid posterior collapse</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a href="https://sanweiliti.github.io/PLACE/PLACE.html">PLACE: Proximity Learning of Articulation and Contact in 3D Environments [3DV'20]</a></h3><time class="published">November, 2020</time></header><a href="https://sanweiliti.github.io/PLACE/PLACE.html" class="image"><img alt="PLACE: Proximity Learning of Articulation and Contact in 3D Environments [3DV'20]" src="/personal-site/images/projects/PLACE-teaser.png"></a><div class="description"><p>Contributions: (1) State-of-the-art technique to put 3D bodies in 3D scenes.(2) Basis point set to model human-scene interactions.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a href="https://github.com/korrawe/grasping_field">Grasping Field: Learning Implicit Representations for Human Grasps [3DV'20, best paper award]</a></h3><time class="published">November, 2020</time></header><a href="https://github.com/korrawe/grasping_field" class="image"><img alt="Grasping Field: Learning Implicit Representations for Human Grasps [3DV'20, best paper award]" src="/personal-site/images/projects/GF-teaser.png"></a><div class="description"><p>Contributions: (1) Simple and effective implicit representation of hand-object interaction.(2) Generating hand grasping given an objects. Comparable to ground truth.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a href="https://ps.is.mpg.de/publications/smpl-x-conditional-vae-prox-scene-constraints">Generating 3D People in Scenes without People [CVPR'20, oral]</a></h3><time class="published">June, 2020</time></header><a href="https://ps.is.mpg.de/publications/smpl-x-conditional-vae-prox-scene-constraints" class="image"><img alt="Generating 3D People in Scenes without People [CVPR'20, oral]" src="/personal-site/images/projects/PSI-teaser.png"></a><div class="description"><p>Contributions: (1) First work to generate various 3D body meshes in 3D scenes.(2) Benchmarking this task with datasets and evaluation metrics.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>Frontal Low-rank Random Tensors for Fine-grained Action Segmentation [arxiv'19]</a></h3><time class="published">December, 2019</time></header><a class="image"><img alt="Frontal Low-rank Random Tensors for Fine-grained Action Segmentation [arxiv'19]" src="/personal-site/images/projects/FLRT-teaser.png"></a><div class="description"><p>Contributions: (1) A novel compact bilinear pooling layer for action segmentation.(2) Theories on how to use random projection to approximate reproducing kernels in Hilbert space.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a href="https://ps.is.tuebingen.mpg.de/publications/bilinear2018">Local Temporal Bilinear Pooling for Fine-Grained Action Parsing [CVPR'19]</a></h3><time class="published">July, 2019</time></header><a href="https://ps.is.tuebingen.mpg.de/publications/bilinear2018" class="image"><img alt="Local Temporal Bilinear Pooling for Fine-Grained Action Parsing [CVPR'19]" src="/personal-site/images/projects/lbp-teaser.png"></a><div class="description"><p>Contributions: (1) Showing that high-order information is useful for fine-grained action segmentation.(2) Lossless data dimension reduction.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>Multi-modal Pain Intensity Recognition based on the SenseEmotion Database [IEEE Trans. Affective Computing]</a></h3><time class="published">January, 2019</time></header><a class="image"><img alt="Multi-modal Pain Intensity Recognition based on the SenseEmotion Database [IEEE Trans. Affective Computing]" src="/personal-site/images/projects/SensePain-teaser.png"></a><div class="description"><p>Contributions: (1) Pain intensity recognition from facial videos.(2) A coherent story including the dataset, methods and benchmarks.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>An Empirical Study Towards Understanding How Deep Convolutional Nets Recognize Falls [ECCV'18, workshop]</a></h3><time class="published">October, 2018</time></header><a class="image"><img alt="An Empirical Study Towards Understanding How Deep Convolutional Nets Recognize Falls [ECCV'18, workshop]" src="/personal-site/images/projects/acvr2018.PNG"></a><div class="description"><p>Contributions: (1) Evaluating how reliable recognizing falling from videos with CNNs.(2) Appearance and optical flow are complementary.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>Human Motion Parsing by Hierarchical Dynamic Clustering [BMVC'18]</a></h3><time class="published">October, 2018</time></header><a class="image"><img alt="Human Motion Parsing by Hierarchical Dynamic Clustering [BMVC'18]" src="/personal-site/images/projects/HDC.jpg"></a><div class="description"><p>Contributions: (1) Unsupervised temporal mocap data segmentation without deep learning method(2) Computational cost is very low.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>Temporal Human Action Segmentation via Dynamic Clustering [arxiv'18]</a></h3><time class="published">August, 2018</time></header><a class="image"><img alt="Temporal Human Action Segmentation via Dynamic Clustering [arxiv'18]" src="/personal-site/images/projects/dc.png"></a><div class="description"><p>Contributions: (1) A new unsupervised clustering algorithm(2) Faster and better than k-means and spectual clustering for time series processing</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>Continuous Activity Understanding based on Accumulative Pose-Context Visual Patterns [IPTA'17]</a></h3><time class="published">December, 2017</time></header><a class="image"><img alt="Continuous Activity Understanding based on Accumulative Pose-Context Visual Patterns [IPTA'17]" src="/personal-site/images/projects/earlyrecog.png"></a><div class="description"><p>Contributions: (1) Recognizing an action before it happens.(2) Providing a fast method for daily living behavior continous understanding.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>Visual Confusion Recognition in Movement Patterns from Walking Path and Motion Energy [ICOST'17]</a></h3><time class="published">October, 2017</time></header><a class="image"><img alt="Visual Confusion Recognition in Movement Patterns from Walking Path and Motion Energy [ICOST'17]" src="/personal-site/images/projects/confusion.png"></a><div class="description"><p>Contributions: (1) Proposing a dataset of people searching items in a cluttered environment.(2) Tracking the human body and estimating the 3D global trajectory.(3) A novel feature of global trajectories and mental confusion understanding.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>The SenseEmotion Database: A Multimodal Database for the Development and Systematic Validation of an Automatic Pain- and Emotion-Recognition System [IAPRW'16]</a></h3><time class="published">December, 2016</time></header><a class="image"><img alt="The SenseEmotion Database: A Multimodal Database for the Development and Systematic Validation of an Automatic Pain- and Emotion-Recognition System [IAPRW'16]" src="/personal-site/images/projects/senseemotion.png"></a><div class="description"><p>Contributions: (1) A comprehensive dataset for pain and emotion recognition.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>Tissue Classification for Laparoscopic Image Understanding Based on Multispectral Texture Analysis [SPIE MI'16]</a></h3><time class="published">December, 2016</time></header><a class="image"><img alt="Tissue Classification for Laparoscopic Image Understanding Based on Multispectral Texture Analysis [SPIE MI'16]" src="/personal-site/images/projects/spie.png"></a><div class="description"><p>Contributions: (1) Real ex-vivo tissue samples for method evaluation.(2) A multispectual local binary pattern descriptor is proposed.</p></div></article></div><div class="cell-container"><article class="mini-post"><header><h3><a>Improving Two-Thumb Text Entry on Touchscreen Devices [CHI'13]</a></h3><time class="published">May, 2013</time></header><a class="image"><img alt="Improving Two-Thumb Text Entry on Touchscreen Devices [CHI'13]" src="/personal-site/images/projects/textentry.png"></a><div class="description"><p>Contributions: (1) Global optimization for layout design(2) Computational modelling according to Fitts Law</p></div></article></div></article></div><section id="sidebar"><section id="intro"><a href="/personal-site/" class="logo"><img alt="" src="/personal-site/images/me.jpg"></a><header><h2>Yan Zhang</h2><p><a href="mailto:yan.zhang@inf.ethz.ch">yan.zhang@inf.ethz.ch</a></p></header></section><section class="blurb"><h2>About</h2><p>Hi, I'm Yan. I would like to design human-centric AI systems to serve people. I am a postdoc researcher at <a href="https://vlg.inf.ethz.ch/">ETH Zurich</a>, and a guest scientist of <a href="https://ps.is.tuebingen.mpg.de/">Perceiving Systems Department, MPI Tuebingen</a>.</p><ul class="actions"><li><a href="/personal-site/resume" class="button">Learn More</a></li></ul></section><section id="footer"><ul class="icons"><li><a href="https://github.com/yz-cnsdqz"><svg aria-hidden="true" class="svg-inline--fa fa-github fa-w-16" data-icon="github" data-prefix="fab" focusable="false" role="img" viewBox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z" fill="currentColor"></path></svg></a></li><li><a href="https://scholar.google.com/citations?user=5VpkLO8AAAAJ&hl=en"><svg aria-hidden="true" class="svg-inline--fa fa-google fa-w-16" data-icon="google" data-prefix="fab" focusable="false" role="img" viewBox="0 0 488 512" xmlns="http://www.w3.org/2000/svg"><path d="M488 261.8C488 403.3 391.1 504 248 504 110.8 504 0 393.2 0 256S110.8 8 248 8c66.8 0 123 24.5 166.3 64.9l-67.5 64.9C258.5 52.6 94.3 116.6 94.3 256c0 86.5 69.1 156.6 153.7 156.6 98.2 0 135-70.4 140.8-106.9H248v-85.3h236.1c2.3 12.7 3.9 24.9 3.9 41.4z" fill="currentColor"></path></svg></a></li><li><a href="https://www.linkedin.com/in/yan-zhang-3119b11a/"><svg aria-hidden="true" class="svg-inline--fa fa-linkedin-in fa-w-14" data-icon="linkedin-in" data-prefix="fab" focusable="false" role="img" viewBox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"></path></svg></a></li><li><a href="https://twitter.com/cnsdqzyz"><svg aria-hidden="true" class="svg-inline--fa fa-twitter fa-w-16" data-icon="twitter" data-prefix="fab" focusable="false" role="img" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z" fill="currentColor"></path></svg></a></li><li><a href="mailto:yan.zhang@inf.ethz.ch"><svg aria-hidden="true" class="svg-inline--fa fa-envelope fa-w-16" data-icon="envelope" data-prefix="far" focusable="false" role="img" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><path d="M464 64H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 400V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V400H48z" fill="currentColor"></path></svg></a></li></ul><p class="copyright">Â© Yan Zhang.</p></section></section></div></div><script src="https://www.google-analytics.com/analytics.js" async></script><script>!function(e){function r(r){for(var n,u,i=r[0],c=r[1],l=r[2],f=0,p=[];f<i.length;f++)u=i[f],Object.prototype.hasOwnProperty.call(o,u)&&o[u]&&p.push(o[u][0]),o[u]=0;for(n in c)Object.prototype.hasOwnProperty.call(c,n)&&(e[n]=c[n]);for(s&&s(r);p.length;)p.shift()();return a.push.apply(a,l||[]),t()}function t(){for(var e,r=0;r<a.length;r++){for(var t=a[r],n=!0,i=1;i<t.length;i++){var c=t[i];0!==o[c]&&(n=!1)}n&&(a.splice(r--,1),e=u(u.s=t[0]))}return e}var n={},o={1:0},a=[];function u(r){if(n[r])return n[r].exports;var t=n[r]={i:r,l:!1,exports:{}};return e[r].call(t.exports,t,t.exports,u),t.l=!0,t.exports}u.e=function(e){var r=[],t=o[e];if(0!==t)if(t)r.push(t[2]);else{var n=new Promise((function(r,n){t=o[e]=[r,n]}));r.push(t[2]=n);var a,i=document.createElement("script");i.charset="utf-8",i.timeout=120,u.nc&&i.setAttribute("nonce",u.nc),i.src=function(e){return u.p+"static/js/"+({}[e]||e)+"."+{2:"a653b565",4:"a0c4899c",5:"ee709ba1",6:"5f952de4",7:"339620d0",8:"a606db79",9:"30cac2eb",10:"ff8d89ec",11:"53c72d43"}[e]+".chunk.js"}(e);var c=new Error;a=function(r){i.onerror=i.onload=null,clearTimeout(l);var t=o[e];if(0!==t){if(t){var n=r&&("load"===r.type?"missing":r.type),a=r&&r.target&&r.target.src;c.message="Loading chunk "+e+" failed.\n("+n+": "+a+")",c.name="ChunkLoadError",c.type=n,c.request=a,t[1](c)}o[e]=void 0}};var l=setTimeout((function(){a({type:"timeout",target:i})}),12e4);i.onerror=i.onload=a,document.head.appendChild(i)}return Promise.all(r)},u.m=e,u.c=n,u.d=function(e,r,t){u.o(e,r)||Object.defineProperty(e,r,{enumerable:!0,get:t})},u.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},u.t=function(e,r){if(1&r&&(e=u(e)),8&r)return e;if(4&r&&"object"==typeof e&&e&&e.__esModule)return e;var t=Object.create(null);if(u.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:e}),2&r&&"string"!=typeof e)for(var n in e)u.d(t,n,function(r){return e[r]}.bind(null,n));return t},u.n=function(e){var r=e&&e.__esModule?function(){return e.default}:function(){return e};return u.d(r,"a",r),r},u.o=function(e,r){return Object.prototype.hasOwnProperty.call(e,r)},u.p="/personal-site/",u.oe=function(e){throw console.error(e),e};var i=this["webpackJsonppersonal-site"]=this["webpackJsonppersonal-site"]||[],c=i.push.bind(i);i.push=r,i=i.slice();for(var l=0;l<i.length;l++)r(i[l]);var s=c;t()}([])</script><script src="/personal-site/static/js/3.76a09191.chunk.js"></script><script src="/personal-site/static/js/main.2d290560.chunk.js"></script></body></html>